# Opik Experiments & UI Mapping Guide

## What Shows Up in Opik Dashboard

This guide maps what happens in your code to what you see in the Opik UI at https://www.comet.com/opik.

---

## Your Code â†’ Opik Dashboard

### 1. **Traces** (Main View)

**What it is**: One trace = one cart creation request

**In your code**:
```python
# api/main.py line 280
orch = Orchestrator(use_llm_extraction=True, use_llm_explanations=True)
result = orch.process_prompt("chicken biryani for 4", servings=4)
```

**In Opik UI**:
```
ğŸ” Traces (List View)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trace Name: process_prompt                                  â”‚
â”‚ Input: "chicken biryani for 4", servings=4                  â”‚
â”‚ Output: 12 items, $78.50 total                             â”‚
â”‚ Duration: 2.3s                                              â”‚
â”‚ Status: âœ… Success                                          â”‚
â”‚ Timestamp: 2026-01-28 18:45:23                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Click on a trace to drill down â†’ see the step-by-step breakdown.

---

### 2. **Spans** (Inside a Trace)

**What it is**: Each agent/step within the orchestrator

**In your code**:
```python
# src/orchestrator/orchestrator.py
def process_prompt(self, user_prompt: str, servings: int):
    with self.tracker.trace("process_prompt"):           # â† Main trace
        self.tracker.track_step("step_ingredients", ...)  # â† Span 1
        self.tracker.track_step("step_candidates", ...)   # â† Span 2
        self.tracker.track_step("step_enrich", ...)       # â† Span 3
        self.tracker.track_step("step_decide", ...)       # â† Span 4
```

**In Opik UI** (drill-down view):
```
ğŸ“Š Trace Timeline: process_prompt (2.3s total)

â”œâ”€ step_ingredients (0.8s)
â”‚  Input: "chicken biryani for 4"
â”‚  Output: 8 ingredients extracted
â”‚  â”œâ”€ LLM Call: claude-sonnet-4 (0.7s)
â”‚  â”‚  Tokens: 450 input, 120 output
â”‚  â”‚  Cost: $0.0032
â”‚  â””â”€ Metadata: use_llm=True
â”‚
â”œâ”€ step_candidates (0.6s)
â”‚  Input: 8 ingredients
â”‚  Output: 48 candidate products
â”‚  Metadata: avg 6 products per ingredient
â”‚
â”œâ”€ step_enrich (0.3s)
â”‚  Input: 48 candidates
â”‚  Output: safety + seasonal signals
â”‚  â”œâ”€ safety_agent (0.2s)
â”‚  â””â”€ seasonal_agent (0.1s)
â”‚
â””â”€ step_decide (0.6s)
   Input: 48 candidates + signals
   Output: 12 final items (3 tiers Ã— 4 ingredients)
   Metadata: tier_distribution={CHEAPER: 4, BALANCED: 4, CONSCIOUS: 4}
```

---

### 3. **LLM Calls** (Nested in Spans)

**What it is**: Every Claude API call made during the request

**In your code**:
```python
# src/llm/ingredient_extractor.py
response = self.client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": prompt}]
)
```

**In Opik UI** (inside a span):
```
ğŸ¤– LLM Call: claude-sonnet-4-20250514

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Provider: Anthropic                              â”‚
â”‚ Model: claude-sonnet-4-20250514                  â”‚
â”‚ Duration: 0.7s                                   â”‚
â”‚ Cost: $0.0032                                    â”‚
â”‚                                                  â”‚
â”‚ Tokens:                                          â”‚
â”‚   Input: 450 tokens ($0.0027)                   â”‚
â”‚   Output: 120 tokens ($0.0005)                  â”‚
â”‚                                                  â”‚
â”‚ Temperature: 0.3                                 â”‚
â”‚ Max Tokens: 1000                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Prompt (click to expand)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Extract ingredients from: "chicken biryani for 4"
Return JSON format...

ğŸ’¬ Response (click to expand)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "ingredients": [
    {"name": "chicken", "quantity": "2 lbs"},
    {"name": "basmati rice", "quantity": "2 cups"},
    ...
  ]
}
```

You can click to see the FULL prompt and response.

---

## Conducting Experiments

### Experiment Setup

**Question**: "How do I test if my new prompt is better than the old one?"

**Answer**: Use Opik's comparison features

#### Approach 1: Tags

**In your code**:
```python
# api/main.py

# Experiment A: Old prompt
orch_a = Orchestrator(use_llm_extraction=True)
with tracker.trace("process_prompt", metadata={"experiment": "prompt_v1"}):
    result_a = orch_a.process_prompt(meal_plan, servings)

# Experiment B: New prompt
orch_b = Orchestrator(use_llm_extraction=True)
with tracker.trace("process_prompt", metadata={"experiment": "prompt_v2"}):
    result_b = orch_b.process_prompt(meal_plan, servings)
```

**In Opik UI**:
```
ğŸ”¬ Filter by metadata: experiment=prompt_v1
   â†’ Shows all traces using old prompt

ğŸ”¬ Filter by metadata: experiment=prompt_v2
   â†’ Shows all traces using new prompt

ğŸ“Š Compare:
   Prompt V1: Avg latency 2.3s, avg cost $0.032
   Prompt V2: Avg latency 1.8s, avg cost $0.028

   Winner: V2 is faster AND cheaper! âœ…
```

#### Approach 2: Projects

Create separate Opik projects for each experiment:

```python
# In .env
OPIK_PROJECT_NAME=conscious-cart-v1  # Baseline
# vs
OPIK_PROJECT_NAME=conscious-cart-v2  # New version
```

Then compare projects side-by-side in Opik.

---

## What to Look For in Opik UI

### 1. **Performance Metrics**

**Navigation**: Dashboard â†’ Traces â†’ Sort by duration

**Look for**:
- **Slow traces**: > 5s (investigate bottleneck)
- **Fast traces**: < 2s (good!)
- **Outliers**: One agent taking 80% of time

**Example**:
```
Trace 1: 8.2s (âš ï¸ WHY SO SLOW?)
â”œâ”€ step_ingredients: 0.8s
â”œâ”€ step_candidates: 0.6s
â”œâ”€ step_enrich: 6.5s  â† ğŸ”´ BOTTLENECK!
â””â”€ step_decide: 0.3s
```

**Action**: Investigate why `step_enrich` is slow. Is FDA API down?

---

### 2. **Cost Tracking**

**Navigation**: Dashboard â†’ Traces â†’ Cost column

**Look for**:
- **Cost per cart**: Should be ~$0.02 with LLM
- **Expensive outliers**: > $0.10 (investigate why)
- **Daily spend**: Track total cost

**Example**:
```
Today's Usage:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
100 carts created
Total cost: $2.10
Avg per cart: $0.021 âœ…

LLM breakdown:
- Ingredient extraction: $1.20 (60%)
- Explanations: $0.90 (40%)
```

**Action**: If cost too high, consider:
- Shorter prompts
- Use Haiku instead of Sonnet
- Cache common queries

---

### 3. **Error Tracking**

**Navigation**: Dashboard â†’ Traces â†’ Filter by status=failed

**Look for**:
- **Failed traces**: Red âŒ indicator
- **Error messages**: Click to see stack trace
- **Failure patterns**: Does "pasta" always fail?

**Example**:
```
âŒ Failed Trace: process_prompt
   Input: "gluten-free pasta for 2"
   Error: KeyError: 'pasta'

   Stack trace:
   File: src/agents/product_agent.py, line 234
   Cause: No products found for "pasta"
```

**Action**: Add pasta to synthetic inventory or handle missing products gracefully.

---

### 4. **Quality Metrics** (with LLM-as-a-Judge)

**In your code**:
```python
# After creating cart
from src.opik_integration.llm_judge import evaluate_recommendation

evaluation = evaluate_recommendation(meal_plan, servings, cart_items, total)

# Log to Opik
tracker.log_feedback(
    trace_id=current_trace_id,
    scores={
        "relevance": evaluation["dimensions"]["relevance"],
        "value": evaluation["dimensions"]["value"],
        "ethics": evaluation["dimensions"]["ethics"],
        "safety": evaluation["dimensions"]["safety"],
        "clarity": evaluation["dimensions"]["clarity"],
        "overall": evaluation["overall_score"]
    }
)
```

**In Opik UI**:
```
ğŸ“ˆ Feedback Scores

Trace: "chicken biryani for 4"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Relevance:  â­â­â­â­â­  5/5  â”‚
â”‚ Value:      â­â­â­â­    4/5  â”‚
â”‚ Ethics:     â­â­â­â­    4/5  â”‚
â”‚ Safety:     â­â­â­â­â­  5/5  â”‚
â”‚ Clarity:    â­â­â­â­    4/5  â”‚
â”‚                             â”‚
â”‚ Overall:    4.2/5           â”‚
â”‚ Verdict:    Excellent âœ…    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

You can then:
- Filter traces by score range (e.g., show all with score < 3)
- Compare average scores across experiments
- Find low-scoring carts to investigate

---

## UI Elements Map

### Main Dashboard
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ  Opik Dashboard                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Projects: [conscious-cart-coach â–¼]                       â”‚
â”‚                                                            â”‚
â”‚  ğŸ“Š Today's Stats:                                        â”‚
â”‚     â€¢ 127 traces                                          â”‚
â”‚     â€¢ 98.4% success rate                                  â”‚
â”‚     â€¢ 2.3s avg latency                                    â”‚
â”‚     â€¢ $2.84 total cost                                    â”‚
â”‚                                                            â”‚
â”‚  ğŸ” Traces (Recent)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ âœ… "chicken biryani for 4" | 2.1s | $0.021       â”‚    â”‚
â”‚  â”‚ âœ… "seasonal veggies"      | 1.8s | $0.019       â”‚    â”‚
â”‚  â”‚ âŒ "gluten-free pasta"     | 0.4s | $0.002       â”‚    â”‚
â”‚  â”‚ âœ… "salmon dinner for 2"   | 2.4s | $0.023       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  Filters: [Status â–¼] [Duration â–¼] [Cost â–¼] [Date â–¼]     â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trace Detail View
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â† Back to Traces                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Trace: process_prompt                                      â”‚
â”‚ ID: tr_abc123xyz                                           â”‚
â”‚ Status: âœ… Success | Duration: 2.1s | Cost: $0.021        â”‚
â”‚                                                            â”‚
â”‚ ğŸ“¥ Input:                                                  â”‚
â”‚    meal_plan: "chicken biryani for 4"                     â”‚
â”‚    servings: 4                                            â”‚
â”‚                                                            â”‚
â”‚ ğŸ“¤ Output:                                                 â”‚
â”‚    items: 12 products                                     â”‚
â”‚    total: $78.50                                          â”‚
â”‚                                                            â”‚
â”‚ â±ï¸  Timeline:                                              â”‚
â”‚  â”œâ”€ [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] step_ingredients (0.8s)                    â”‚
â”‚  â”œâ”€ [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] step_candidates (0.6s)                     â”‚
â”‚  â”œâ”€ [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] step_enrich (0.3s)                         â”‚
â”‚  â””â”€ [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] step_decide (0.4s)                         â”‚
â”‚                                                            â”‚
â”‚ ğŸ¤– LLM Calls:                                              â”‚
â”‚  â€¢ claude-sonnet-4: 450 in, 120 out | $0.0032            â”‚
â”‚  â€¢ claude-sonnet-4: 780 in, 340 out | $0.0056            â”‚
â”‚                                                            â”‚
â”‚ ğŸ“Š Feedback:                                               â”‚
â”‚  â€¢ Overall Score: 4.2/5                                   â”‚
â”‚  â€¢ Verdict: Excellent                                     â”‚
â”‚                                                            â”‚
â”‚ [View Full Trace JSON] [Compare] [Export]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison View
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compare Experiments                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Experiment A: prompt_v1 (50 traces)                      â”‚
â”‚  Experiment B: prompt_v2 (50 traces)                      â”‚
â”‚                                                            â”‚
â”‚  Metric         â”‚ Prompt V1  â”‚ Prompt V2  â”‚ Î”           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Avg Latency    â”‚ 2.3s       â”‚ 1.8s       â”‚ -22% ğŸŸ¢     â”‚
â”‚  Success Rate   â”‚ 96%        â”‚ 98%        â”‚ +2% ğŸŸ¢      â”‚
â”‚  Avg Cost       â”‚ $0.032     â”‚ $0.028     â”‚ -13% ğŸŸ¢     â”‚
â”‚  Quality Score  â”‚ 4.1/5      â”‚ 4.3/5      â”‚ +0.2 ğŸŸ¢     â”‚
â”‚                                                            â”‚
â”‚  Winner: Prompt V2 is better across all metrics! âœ…       â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Example Experiment Workflow

### Scenario: Test if removing LLM extraction improves speed

**Step 1: Baseline** (current state)
```python
# api/main.py
orch = Orchestrator(
    use_llm_extraction=True,   # Uses Claude to parse
    use_llm_explanations=True
)

# Add metadata
with tracker.trace("process_prompt", metadata={"experiment": "llm_extraction_on"}):
    result = orch.process_prompt(meal_plan, servings)
```

Run 50 test carts, note in Opik:
- Avg latency: 2.3s
- Avg cost: $0.032
- Success rate: 98%

**Step 2: Experiment** (new approach)
```python
# api/main.py
orch = Orchestrator(
    use_llm_extraction=False,  # Uses templates instead
    use_llm_explanations=True
)

# Add metadata
with tracker.trace("process_prompt", metadata={"experiment": "llm_extraction_off"}):
    result = orch.process_prompt(meal_plan, servings)
```

Run 50 test carts, note in Opik:
- Avg latency: 1.1s (âš¡ 52% faster!)
- Avg cost: $0.012 (ğŸ’° 62% cheaper!)
- Success rate: 92% (âš ï¸ 6% worse)

**Step 3: Compare in Opik**
1. Go to Traces
2. Filter: `metadata.experiment=llm_extraction_on`
3. Export metrics
4. Filter: `metadata.experiment=llm_extraction_off`
5. Export metrics
6. Compare side-by-side

**Step 4: Decision**
- Templates are faster and cheaper âœ…
- But 6% lower success rate âŒ
- **Decision**: Use templates for known recipes, LLM for complex requests

---

## Quick Reference

### What Opik Tracks Automatically
âœ… Every trace (cart creation)
âœ… Every span (agent step)
âœ… Every LLM call (with tokens/cost)
âœ… Duration for each step
âœ… Input/output for each step
âœ… Success/failure status
âœ… Timestamps

### What You Need to Add Manually
âš ï¸ Experiment tags (`metadata={"experiment": "v2"}`)
âš ï¸ Custom metrics (quality scores)
âš ï¸ Business metrics (user satisfaction, conversion rate)
âš ï¸ Feedback loops (LLM-as-a-Judge scores)

### Where to Look For...

| What You Want | Where in Opik UI |
|--------------|------------------|
| Slow requests | Traces â†’ Sort by duration DESC |
| Failed requests | Traces â†’ Filter status=failed |
| Expensive requests | Traces â†’ Sort by cost DESC |
| LLM prompt/response | Trace detail â†’ LLM Calls â†’ Click to expand |
| Compare experiments | Traces â†’ Group by metadata â†’ Export |
| Daily cost | Dashboard â†’ Cost graph |
| Quality trends | Traces â†’ Feedback scores |

---

## Best Practices

1. **Always tag experiments**
   ```python
   metadata={"experiment": "prompt_v3", "date": "2026-01-28"}
   ```

2. **Log quality scores**
   ```python
   tracker.log_feedback(trace_id, scores={"overall": 4.2})
   ```

3. **Use consistent naming**
   - trace name: always `"process_prompt"`
   - span names: `"step_ingredients"`, `"step_candidates"`, etc.

4. **Filter strategically**
   - Time range: Last 24 hours (for daily monitoring)
   - Status: Failed only (for debugging)
   - Metadata: experiment=v2 (for A/B tests)

5. **Export data for analysis**
   - Opik has export â†’ CSV feature
   - Load into Python/Excel for deeper analysis
